<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=windows-1252">
    <meta name="Generator" content="Microsoft Word 97">
    <style type="Text/css">
<!- A:Hover {Text-Decoration: None; Color: Blue;}-></style>
    <title>AI</title>
  </head>
  <body vlink="#ff8080" text="#000000" link="#ff0000" bgcolor="#ffffff">
    <p>
      <table align="center" width="400" cellpadding="2" bgcolor="black">
        <tbody>
          <tr>
            <td bgcolor="teal">
              <h2 align="Center"> <font color="#ffffff">Technology</font> </h2>
            </td>
          </tr>
        </tbody>
      </table>
    </p>
    <p>
    </p>
    <h2 align="CENTER"> <u>The New Thinking Machines</u>
    </h2>
    <p>
      <table align="center" width="330" cellpadding="2">
        <tbody>
          <tr>
            <td><img src="ai/AI-1.gif" height="223" width="299"></td>
          </tr>
          <tr>
            <td>
              <h3> <b>Does thought have to be an exclusively human activity?
                  Ironically,every stage of the debate has helped artificial
                  intelligence become a reality.</b> </h3>
            </td>
          </tr>
        </tbody>
      </table>
    </p>
    <p>
      Build an artificial intelligence, a computer that thinks like a person,
      and
      - as every reader of science fiction knows - it soon wants to take over
      the
      world. The reality, of course, is very different. Just as
      <a href="../Templarser/metro36.html">robot</a>-builders have dismally
      failed
      to reproduce a whole human body, so researchers in artificial intelligence
      (AI) have created only fragments of human intelligence. These creations
      are
      far stranger than the destructive artificial minds of science fiction, but
      also far more useful.<br>
    </p>
    <p>
      <table align="left" width="107" cellpadding="2">
        <tbody>
          <tr>
            <td><img src="ai/A-TURING.gif" height="107" width="100"></td>
          </tr>
          <tr>
            <td><font size="2"><b> 'By the end of the century one will be able
                  to speak of machines thinking without being contradicted' <i><a

                      href="http://www-groups.dcs.st-and.ac.uk/%7Ehistory/Mathematicians/Turing.html">Alan
                      Turing</a>, 1950</i></b></font></td>
          </tr>
        </tbody>
      </table>
    </p>
    <p>
      The irony of artificial intelligence is that tasks that are hard for
      people
      - like playing good chess, solving logic problems and juggling complex
      sets
      of rules - are relatively easy for machines. But common-sense tasks - like
      getting to work, holding a conversation, or avoiding danger - are still
      far
      beyond the capability of even the smartest machines. <br>
    </p>
    <p>
      Artificially intelligent computers can beat a grandmaster at chess, but
      would
      sit pondering their next move as the room burned down around them. In the
      factory, artificial intelligences manage production schedules
      incomprehensible
      to people - yet they couldn't get to work in the morning if they had to.
      Computers
      "read" and route paperwork in banks and offices, but cannot understand a
      child's first book. <br>
    </p>
    <p>
      Are such machines "intelligent" then? Certainly they fail by the criterion
      proposed by computer pioneer <a href="../../stationx.html">Alan Turing</a>.
      Intelligence,he reckoned,is in the eye of the beholder. A machine could be
      deemed intelligent only if it could hold a conversation with a person
      (through
      a telex machine if necessary, to disguise the fact that the machine lacked
      a body) and convince the person that it was human. No machine has passed
      that test and none looks likely to for quite some time. <br>
    </p>
    <p>
      <table align="right" width="300" cellspacing="4" cellpadding="4" bgcolor="#505040">
        <tbody>
          <tr>
            <td bgcolor="#606050"><font color="White"><font size="2"><b> Make
                    your move, robot<br>
                  </b> Chess is hard for humans to play well, but machines can
                  beat most people easily. This robot is capable of dismissing a
                  good club player using only a few seconds of computing time.
                  Yet, despite its humanoid shape, it is less able to move about
                  than a young toddler. So how intelligent is that?</font></font></td>
            <td>
              <p align="Center"> <img src="ai/AI-2.gif" height="240" width="147"></p>
            </td>
          </tr>
        </tbody>
      </table>
    </p>
    <p>
    </p>
    <p align="Justify">
      But it is precisely because machines do not replicate human abilities that
      they are so potentially useful. After all, there are plenty of people and,
      should a shortage arise, there is a well-known technique for creating
      more.
      <br>
      In today's information economy, any unusual reasoning ability is valuable.
      Although the extraordinary skills of computers in some sorts of reasoning
      are offset by equally extraordinary stupidity in other areas, by building
      the right partnership between human and machine it is possible to offset
      computer stupidity with human intelligence, and vice versa.
    </p>
    <p>
      At the beginning of each era of its historical development, Al has thrown
      up at least one good new idea about the underlying roots of intelligence.
      By the end of the era the idea is found to be at best only a partial
      solution
      - but a partial solution with some potentially useful applications
      nevertheless.
    </p>
    <h3>
      <p align="Center"> <font color="Maroon">The first stirrings of artificial
          intelligence</font><br>
      </p>
    </h3>
    <p align="Left">
      The first era started in the late 1950s, when Herbert Simon, a professor
      at Pittsburgh's Carnegie Mellon University (who was later to win a Nobel
      prize for economics), optimistically named his "General Problem Solver".
      <br>
      Simon and his colleague Alan Newell had spent hours getting students to
      talk
      aloud as they solved simple reasoning problems. By analysing the
      mutterings,
      they reckoned they had found the general principles underlying
      intelligence:
      in short, they thought it was all about finding ways to sift through all
      the known possibilities to arrive at the right answer.
    </p>
    <p align="Left">
      <br>
      <table align="center" width="450" cellspacing="4" cellpadding="4" bgcolor="#fff0ff">
        <tbody>
          <tr>
            <td colspan="2">
              <h3 align="Center"> Smart machines go to&nbsp;the movies </h3>
            </td>
            <td rowspan="2"><img src="ai/ROBBIE.gif" height="287" width="143"></td>
          </tr>
          <tr>
            <td>
              <table cellpadding="2" bgcolor="#f08040">
                <tbody>
                  <tr>
                    <td>
                      <p align="Center"> <img src="ai/FORBIN.gif" height="154"

                          width="180"></p>
                    </td>
                  </tr>
                  <tr>
                    <td><font color="Purple"><font size="2"><b>Expert systems
                            hold the nuclear balance</b><br>
                        </font></font><font size="2"> In the early 1970s film <i>The
                          Forbin Project</i>,both America and&nbsp;the USSR
                        entrust their strategic defence to intelligent
                        computers. Naturally, the two machines gang up on the
                        human race. </font></td>
                  </tr>
                </tbody>
              </table>
            </td>
            <td bgcolor="#f03030"><img src="ai/HAL.gif" height="187" width="100"></td>
          </tr>
          <tr>
            <td colspan="3"><font color="Black"><font color="Purple"><font color="Purple"><font

                      size="2"><b> Artificial Intelligence on a bad day</b></font></font><font

                    size="2"> </font></font><font size="2">The sentient
                  computer in <i>2001: A Space Oddysey</i>, <a href="http://www.fortunecity.com/emachines/e11/86/behind3.html">HAL</a>
                  (centre), is driven mad by conflicting orders. Today, it is
                  more likely for a human not to understand an AI than the other
                  way around.<br>
                </font></font><font color="Purple"><font size="2"> <b>Stand up
                    for robot rights </b></font></font><font size="2">Robbie
                (right) was star of the 1950s sci-fi classic <i>Forbidden
                  Planet</i>. Able to drive a car, serve drinks and hold a
                pleasant conversation, he would easily have passed the Turing
                test. Although he opened shops and had his own series, Robbie
                was still a man in a suit. We are never likely to see his like.
              </font></td>
          </tr>
        </tbody>
      </table>
    </p>
    <p align="Left">
    </p>
    <p align="Left">
      Simon and Newell believed a machine could carry out this procedure, which
      they called "means-end analysis". If a mechanism could generate, step by
      step, every possible solution, it would effectively reproduce
      intelligence.<br>
      For their machine to solve a problem, it first had to figure out how to
      know
      when it had found the answer. One type of problem studied by Simon and
      Newell
      was the cryptarithmetic puzzle, for example "SEND + MORE = MONEY". You've
      got the solution when you know which digits substitute for which letters
      in the equation.<br>
    </p>
    <p align="Left">
      <table align="left" width="250" cellpadding="2" bgcolor="#f0f0ff">
        <tbody>
          <tr>
            <td colspan="2"><img src="ai/AI-4.gif" height="147" width="250"></td>
          </tr>
          <tr>
            <td><font color="Purple"><b>Machines adapt to changes of plan easily
                  - but they fail other tests that humans pass</b></font></td>
            <td>
              <p align="Center"> <img src="ai/AI-3.gif" height="208" width="150"></p>
            </td>
          </tr>
          <tr>
            <td colspan="2"><font size="2"> Planning by computer means you can
                adapt to any change instantly. If a shipment is held up at the
                docks (above) the computer can reschedule production. The same
                AI system can make sure best use is made of warehouse space
                (top)</font></td>
          </tr>
        </tbody>
      </table>
    </p>
    <p align="Left">
      Then the General Problem Solver would look for means to achieve that end.
      It might begin by trying to substitute one for S and see if that could be
      the first step towards a solution. If it wasn't, the machine would try
      two,
      and so on. Having tried out all the means, the machine selects only the
      ones
      that meet its goals.
    </p>
    <p align="Left">
      <br>
      The problem is that it can, and typically does, take so long for the
      computer
      to find a solution as to render it useless. A computer searching for a way
      to get to work on time is wrong if arrives at lunchtime - even if it has
      found a more cunning way than any previously devised. And, as can be shown
      mathematically, there are some problems that would require at least the
      age
      of the known universe for a computer to solve by the means-end method.<br>
      There are, however, techniques for narrowing down the search. In our
      cryptarithmetic problem, for example. even a fairly simple- minded
      computer
      could be programmed to spot that the letter to start with is M, as it is
      the first digit of two of the terms, one of which is the answer. (It must
      equal one as, no matter what each digit is, two of them added together
      will
      not equal more than 18: so only a one will ever be carried over.) <br>
    </p>
    <p align="Left">
      For a few problems, a clever search will outdo even the best human
      performance.
      Chess is probably the best example. The <a href="../../chess.html">computers
that
        now routinely beat grandmasters </a>have no grasp of strategy at all.
      Unlike humans, they can analyse several million possible chess positions
      a second, and score each according to some simple measure. In this way the
      computer can look six, eight, even ten moves into the future. In chess,
      the
      key to narrowing the search is to assume that the opponent will always
      make
      the best move available. Once the computer finds the replies that would
      leave
      it worse off, it can dismiss any move that allows the opponent to make
      those
      replies. This facility for sifting through vast realms of possibilities
      for
      the best combination of steps towards a given goal gives machines many
      planning
      applications. Computers helped with the logistics of moving men and
      equipment
      into place for the Gulf war. Another military use for AI is organising
      maintenance schedules for aircraft. Black and Decker uses Alto schedule
      its
      production of tools.
    </p>
    <p align="Left">
      <br>
      The big advantage of planning by computer is the ability to keep track of
      interactions between the various parts of a production plan. If, say, a
      shipment
      of parts is delayed, the computer can quickly reschedule production to
      make
      sure machines do not sit idle. Despite their successes at chess and
      planning,
      today's computers cannot solve most other kinds of problems. Computers
      cannot
      play the Japanese game Go, for example, because at each turn there are
      many
      more moves available to each player than in chess - the sheer number of
      possibilities quickly becomes overwhelming. To overcome these
      difficulties,
      researchers in the 1960s and 1970s began trying to mimic the ways humans
      use knowledge to reason their way directly to an answer, instead of
      detouring
      through all sorts of unlikely possibilities.
    </p>
    <p align="Left">
      <table align="center" width="500" cellspacing="4" cellpadding="4" bordercolor="gray"

        border="4" bgcolor="#fffff0">
        <tbody>
          <tr>
            <td bgcolor="maroon">
              <p align="Center"> <font color="White"><b>Expert
                    Systems:&nbsp;questions&nbsp;and answers</b></font></p>
            </td>
          </tr>
          <tr>
            <td>
              <table align="left" width="210" cellpadding="2" bgcolor="#e0f0ff">
                <tbody>
                  <tr>
                    <td colspan="2"><img src="ai/AI-6.gif" height="161" width="200"></td>
                  </tr>
                  <tr>
                    <td colspan="2"><font size="2"><b>Planes and automobiles <br>
                        </b>There are only a limited number of factors to juggle
                        when controlling air traffic (left),</font></td>
                  </tr>
                  <tr>
                    <td>
                      <p align="Center"> <img src="ai/AI-5.gif" height="181" width="119"></p>
                    </td>
                    <td><font size="2">but a human brain alone would make
                        mistakes.<br>
                        AI also takes the guesswork out of tuning a car (below).<br>
                      </font></td>
                  </tr>
                </tbody>
              </table>
              <p> <font size="2"> The classic example of what expert systems do
                  is provided by medical diagnosis computers, which re-create
                  the reasoning doctors use in diagnosing and prescribing a
                  cure. An early expert system was MYCIN, developed in the late
                  1960s by Ed Shortliffe at Stanford University, which
                  specialised in diagnosing bacterial infections and prescribing
                  antibiotic cures. <br>
                </font> </p>
              <p> <font size="2">Diagnosing disease involves mastering a
                  collection of fairly simple rules. One rule might be: "If the
                  patient has a fever, consider the following list of
                  diseases..." Another might be: "If the patient has white spots
                  on the throat, consider..." Whereas doctors laboriously learn
                  hundreds of such rules in medical school, a computer can be
                  programmed in a jiffy to apply them. </font> </p>
              <h4> <font size="2"> 'When an expert system messes up, it does so
                  big time - they have to be supervised as well as specialised'<br>
                </font> </h4>
              <p> <font size="2">Not surprisingly, they do it very well. Since
                  the 1970s, expert systems have on average outperformed doctors
                  in diagnosing most diseases. Computers have faultless memories
                  and iron-clad logic when it comes to combining rules. But not
                  only do expert systems lack bedside manner, they also have
                  important faults that have excluded them from widespread use
                  in medicine.<br>
                </font> </p>
              <p>
                <table align="right" width="247" cellpadding="2" bgcolor="#204080">
                  <tbody>
                    <tr>
                      <td><img src="ai/AI-7.gif" height="164" width="247"></td>
                    </tr>
                    <tr>
                      <td><font color="White"><font size="2"><b>Machines that
                              tell the human body what's wrong with it</b><br>
                            Computers are more accurate than human doctors at
                            diagnosing illness but they need to be constantly
                            fed information - and mistakes can happen.<br>
                          </font></font></td>
                    </tr>
                  </tbody>
                </table>
              </p>
              <p> </p>
              <ul>
                <li> <font size="2"><b>Narrow knowledge </b>Computers easily
                    cope with 50 to 100 rules, but any more confuses them. Just
                    as MYCIN was restricted to bacterial infections, even today
                    expert systems are specialists, useful in a narrow realm
                    only. <br>
                    When expert systems mess up, they do so spectacularly,
                    falling off their narrow platform of knowledge into
                    incompetence. This is particularly troubling in medicine:
                    the achievement of an expert system that treated 96 per cent
                    of patients brilliantly would be quickly overshadowed if it
                    killed the remaining four per cent. </font> </li>
                <li> <font size="2"><b>Communications</b> Given this
                    narrowness, expert systems cannot be left to work
                    unsupervised - and this is a real disadvantage. The constant
                    input needed to keep human and machine working in harmony is
                    too time-consuming, particularly for doctors. This is not
                    true in other realms. <br>
                    Expert systems are now routinely deployed throughout banking
                    and industry. They detect credit card fraud and help brokers
                    pick investments for clients. For car-makers they pick up
                    engine faults and administer the complicated rules governing
                    eligibility under warranty. And in customer relations they
                    pass on requests for &nbsp;help to the right person. <br>
                    Expert systems have proved to be good coaches in areas where
                    the difference between a newcomer and an experienced hand is
                    great, but the knowledge mastered with experience
                    can&nbsp;be distilled into a few rules that change little
                    over time. In fact,most&nbsp;users never know they&nbsp;are
                    working with an expert system; they just think new computer
                    is particularly user-friendly.<br>
                  </font> </li>
              </ul>
            </td>
          </tr>
        </tbody>
      </table>
    </p>
    <p>
      <table align="center" width="500" cellspacing="4" cellpadding="4" bordercolor="gray"

        border="4" bgcolor="#fffff0">
        <tbody>
          <tr>
            <td bgcolor="maroon">
              <p align="Center"> <font color="White"><b>Neural Networks: from
                    mind to brain </b></font></p>
            </td>
          </tr>
          <tr>
            <td>
              <table align="left" width="210" cellpadding="2" bgcolor="#e0f0ff">
                <tbody>
                  <tr>
                    <td><img src="ai/AI-8.gif" height="150" width="220"></td>
                  </tr>
                  <tr>
                    <td><font size="2"><b> Pattern recognition</b> A net looks
                        for abnormalities in brain waves (above). The same
                        technology identifies planes from any angle (right) -
                        and may soon be able to predict a pilot's next move.</font></td>
                  </tr>
                  <tr>
                    <td>
                      <p align="Center"> <img src="ai/AI-9.gif" height="143" width="220"></p>
                    </td>
                  </tr>
                </tbody>
              </table>
              <p> <font size="2"> Neural networks are the closest thing to
                  voodoo yet cre ated by the "<a href="../../magic.html">white
                    magic</a>" of computer science. Data goes in and information
                  comes out, but what happens in between is a bit of a mystery.
                  Also, they "learn".<br>
                  While expert systems are modelled on the way people think -
                  the human mind - neural networks take as their starting point
                  the human brain. A neural net is composed of hundreds (or
                  thousands) of simple "nodes", roughly analogous to the brain's
                  neurons. These nodes are arranged in layers. Every node has
                  several inputs from the previous layer, and an output to the
                  next layer.<br>
                  Each of these inputs is an influence rather than a straight-
                  forward yes/no - one input might say "almost certainly", while
                  two others indicate "perhaps not". The node "fires" an output
                  if the sum of the inputs exceeds its particular threshold
                  (which varies between nodes). </font> </p>
              <h4> <font size="2"> 'Because it learns in mysterious ways and
                  cannot explain itself a neural net must be taken on trust-or
                  not at all'<br>
                </font> </h4>
              <p> <font size="2"> The output is combined with other node
                  outputs from the same layer to become the input to the nodes
                  on the next layer. A network learns by adjusting the
                  thresholds of each of its nodes-changing how readily they fire
                  - and by rerouting the interconnections between the nodes and
                  layers. <br>
                  Suppose you want to make a colour photocopier that can't be
                  used to copy banknotes. You could build a memory containing
                  images of every note in the world, and update it regularly. Or
                  you could train a neural network. As all banknotes have
                  similarities, once a neural network had learnt to recognise
                  enough banknotes it would be able to recognise notes that it
                  has never seen before.<br>
                </font> </p>
              <p>
                <table align="right" width="196" cellpadding="2" bgcolor="#204080">
                  <tbody>
                    <tr>
                      <td><img src="ai/AI-10.gif" align="Right" height="151" width="196"></td>
                    </tr>
                    <tr>
                      <td><font color="White"><font size="2"><b>A ghost at the
                              wheel</b> <br>
                            One of the most impressive things a neural network
                            can do is drive. An army truck developed at Carnegie
                            Mellon University, learnt to copy a human driver -
                            road conditions are input by video and laser
                            rangefinder.<br>
                          </font></font></td>
                    </tr>
                  </tbody>
                </table>
              </p>
              <p> </p>
              <ul>
                <li> <font size="2"><b> The art of net training</b> It has been
                    mathematically shown that neural nets can learn almost
                    anything if given long enough. The problem is how to tell
                    the network what it's doing right, so it can preserve that,
                    and what it's doing wrong, so it can try something else.
                    There is still a fair amount of art involved in this
                    process. <br>
                    Neural nets started in the 1980s when David Rumelhart, James
                    McLelland and other researchers decided to try to overcome
                    the limitations of artificial intelligence techniques by
                    modelling computers on the complex interconnections of the
                    human brain. As usual, they succeeded, but only partly. <br>
                    Because neural networks learn by experience and not by
                    programmed rules, they bring much more information to bear
                    on a problem than expert systems can and are much better at
                    recognising complex patterns. But whereas an expert system
                    can explain why it has come to a certain conclusion, a
                    neural network must be taken on trust - or not at all. </font>
                </li>
                <li> <font size="2"><b> Unknown factors</b> As a result, neural
                    nets can be misleading. In training, a network learned to be
                    100 per cent accurate in recognising camouflaged tanks. But
                    in the field, it failed completely. Researchers eventually
                    realised that all the training videos with tanks hidden in
                    them had been slightly darker than those without - the
                    vastly complicated neural net had only learned to tell light
                    from dark. <br>
                  </font> </li>
              </ul>
            </td>
          </tr>
        </tbody>
      </table>
    </p>
    <h3 align="Center"> <font color="Maroon">Logical thinking <br>
      </font>
    </h3>
    <p align="Left">
      While <a href="taylor.html">neural networkers</a> have tried to re-create
      the human. brain, and some programmers have created expert systems (see
      boxes
      above), others have been <a href="../../physonb.html">exploring the
        abstract
        realm of mind</a>. Since Greek times, philosophers have worked to create
      hard rules of reasoning, called <a href="../../wotlogic.html">logic</a>.
      For most of its history, <b>logic's goal has been to discover largely
        abstract
        eternal truths through formally constructed arguments</b>. <br>
    </p>
    <p align="Left">
      The advent of artificial intelligence inspired people to try to use formal
      arguments to describe the messy world around them. Two developments helped
      greatly in bringing logic into the real world. The first, which began
      early
      in the 20th century, was modal logic. <br>
      Whereas <a href="../../ptlytrue.html">classical logic</a> assumes a
      statement
      is either true or false for all eternity, <b>modal logic </b>can cope
      with
      statements that may be true at one time but false at another - for
      example,
      "John Major is Prime Minister", which is true now, but won't always be. <br>
      The second development, <b>non-monotonic logic</b>, was largely developed
      by artificial-intelligence researchers in the 1970s and 1980s.
      Non-monotonic
      logic addresses the related problem of incomplete information. In day-
      to-day
      life, we often make useful assumptions that later turn out to be wrong.
      "Birds
      fly" is a good assumption, even if you later discover penguins and roast
      turkeys. <br>
      In modal logic, if an assumption is always false - for example "John Major
      is a woman" - then the whole system collapses and all its conclusions fall
      apart. Non-monotonic logic, by contrast, copes gracefully if the bird
      turns
      out to be flightless later on. Any conclusions that depend on the
      erroneous
      assumption are withdrawn and the rest are still true.
    </p>
    <p align="Left">
      <table align="center" cellpadding="2">
        <tbody>
          <tr>
            <td><font color="Purple"><b>Every time a computer fails in a new job
                  we learn something more about human reason</b></font></td>
            <td bgcolor="#f08040"><font size="2">AI and the law AI helps judges
                deal with petty criminals - freeing courtrooms for important
                cases</font></td>
            <td bgcolor="#f08040">
              <p align="Center"> <img src="ai/AI-12.gif" height="150" width="250"></p>
            </td>
          </tr>
          <tr>
            <td bgcolor="#a0afa0"><img src="ai/B-RUSSL.gif" height="177" width="120"></td>
            <td bgcolor="#a0afa0"><img src="ai/AI-11.gif" height="160" width="150"></td>
            <td rowspan="2"><font size="2"><b> Will computers ever be able to
                  think like a human? </b><br>
                The processors of a computer's chip-brain are made up of
                hundreds of "<a href="../Templarser/elect-logic.html">logic
                  gates</a>" which only understand 0 or 1. If both inputs (A and
                B) to a gate are 1, then the output will be 1; if one input is 1
                (A or B), the output will also be 1.<br>
                This "<a href="http://en.wikipedia.org/wiki/Boolean_algebra">Boolean</a>"
                logic - from the 19th-century English theorist <b>George Boole</b>
                - is powerful. But now computer scientists are developing
                non-binary alternatives. "Multivalued" logic works with inputs
                and outputs that take more than two values. <a href="fuzzy.html">Fuzzy
                  logic </a>operates across a spectrum from "no", through
                "possibly" to "almost certainly" and "yes". The prospect is for
                computers that think in a less black-and- white way - but that
                doesn't necessarily mean more like a human being. </font></td>
          </tr>
          <tr>
            <td colspan="2" bgcolor="#a0afa0"><font size="2"> Logic that washes
                Bertrand Russell (left) helped end the rule of abstract
                classical logic. Now there are many different "logics" - fuzzy
                logic is used to program advanced washing machines (right)</font></td>
          </tr>
        </tbody>
      </table>
    </p>
    <h3>
      <p align="Center"> <font color="Maroon"><br>
          Thinking like a human <br>
        </font>
      </p>
    </h3>
    <p align="Left">
      These new thinking tools spurred logicians to try to capture more of the
      subtleties of real life in their formal statements. This is no mean feat.
      To reduce humans' instinctive understanding of the world to formal logic,
      logicians must cope with a variety of philosophical conundrums. Wood, for
      example, is still wood when it is cut up into bits, but a cut-up table is
      just so much junk. <br>
      What other things are like wood, and how do we distinguish them from
      things
      like tables whose identity seems more fragile? Such questions have added
      fuel to the larger debate about whether the real world - or indeed human
      intelligence - can be reduced to logical terms. Some argue that the world
      is fundamentally illogical, and that to reason as humans requires a mass
      of arbitrary assumptions and leaps of reasoning derived from the
      experience
      of living in a body, in time, in the world. To which the logicians reply
      that although no system of formal reasoning can capture the subtleties of
      human thought, <b>computers would be no use at all without the guarantee
        such a formal system provides.</b><br>
    </p>
    <p align="Left">
      It is out of the see-sawing progress of this argument over computers'
      abilities
      that progress in the field of artificial intelligence emerges. Each
      advance
      in formalising some aspect of human reasoning (or the world) enables
      computers
      to do some new job. And with each new task computers take on, new
      limitations
      become apparent.<br>
    </p>
    <h3 align="Center"> <font color="Maroon"> Fuzzy logic <br>
      </font>
    </h3>
    <p align="Left">
      Even flexible non-monotonic logic treats a statement about the world in
      terms
      of either true or false. In practice, however, many things are "sort of"
      true. Lofti Zadeh, a professor at the University of California at
      Berkeley,
      developed a kind of reasoning that uses "<a href="fuzzy.html">fuzzy</a>"
      terms. Japanese researchers promptly adopted the new techniques to improve
      controls for washing machines. Instead of trying to define just how dirty
      is "dirty", fuzzy controls enable a machine to treat, say, an 80 per cent
      full load of clothes as if they are about 40 per cent dirty. <br>
    </p>
    <h3 align="Center"> <font color="Maroon">Case-based reasoning <br>
      </font>
    </h3>
    <p align="Left">
      Humans learn from experience: whatever worked in the past may work in the
      future. But "case-based" reasoning enables one person to benefit from the
      experience of others. When a problem is solved, a computer files away the
      solution and indexes it by a description of the situation. Faced with a
      new
      problem, you enter a description and see if anything like it has been
      tackled
      before.<br>
      Compaq, a maker of personal computers, uses this technique to keep its
      technical
      support staff abreast of what can go wrong. <br>
    </p>
    <h3 align="Center"> <font color="Maroon">Legal reasoning<br>
      </font>
    </h3>
    <p align="Left">
      Building on research by Imperial College, London, courts in New York and
      Singapore now use AI in straightforward cases. Not only do the computers
      speed up justice, they also make intelligent suggestions about
      rehabilitation.
      <br>
      Each addition to the grab-bag of artificial intelligence techniques
      broadens
      the range of tasks computers can perform; it also pushes us towards a
      fundamental
      philosophical question. Will all these pieces of intelligence ever add up
      to the mental equivalent of a human being? Needless to say, philosophers
      debate the question heavily. The litmus test of philosophical opin ion is
      a thought experiment pro posed by thinker John Searle. Imagine a man who
      lives in a tin' enclosed room. He speaks only English, but in the room is
      a huge Chinese dictionary. Each morning, someone pushes under the door a
      set of tiles, also written in Chinese. The man's job is to rearrange the
      incomprehensible symbols according to the patterns given in the
      dictionary,
      and then each evening, to pass the tiles back through the door. Is the
      man's
      work intelligent? <br>
    </p>
    <p align="Center">
      Some argue that the mans work cannot be considered intelligent because he
      has no inkling of the meaning of the tiles - and therefore
      symbol-processing
      computers cannot be intelligent either. Others reply that humans are
      overconfident when the argue that they "understand' words and sense
      impressions
      and more than a computer does. The debate is a deep and intriguing one,
      but
      it may prove irrelevant in the end. From this point of view of the person
      outside the room, after all, it doesn't really matter if the work going or
      inside is intelligent or meaningful - so long as it is useful. <br>
      <i>John Browning
      </i><br>
      <br>
      <a href="https://www.ted.com/talks/jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn"

        target="_blank">TED TALK Jeremey Howard</a> Computers that
      learn<br>
      <a href="https://www.ted.com/talks/sebastian_thrun_and_chris_anderson_the_new_generation_of_computers_is_programming_itself"

        target="_blank">TED TALK Sebastian Thrun</a> What AI is - and
      isn't.<br>
      <a href="https://www.newscientist.com/article/mg22429932-200-computer-with-human-like-learning-will-program-itself/"

        target="_blank">New Scientist Computer with human-like learning will
        program
        itself</a>
    </p>
    <p align="Center">
      <br>
      <a href="index.html">INDEX</a>
    </p>
    <p> </p>
    <hr>
    <p>
      <img src="FOCUS.gif" align="Middle" height="29" width="80"> Mar95 p30<br>
      <font size="1"> <b>File Info:</b> Created <i>14/8/2001 </i>Updated
        <i>16/8/2001</i> <b>Page Address:</b> </font>
    </p>
    <p>
      <font size="2"></font>
    </p>
  </body>
</html>
